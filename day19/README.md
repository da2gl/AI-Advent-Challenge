# Day 19: Document Indexing with ChromaDB

**Build a complete document indexing pipeline with semantic search capabilities**

## Overview

This project implements a 4-stage document indexing pipeline that enables semantic search over your documents using embeddings and vector similarity. It's the foundation for building RAG (Retrieval-Augmented Generation) systems.

## Key Features

### ğŸ”„ 4-Stage Pipeline
- **Stage 1: Document Loading** - Multi-format support (`.md`, `.rst`, `.txt`, `.py`, `.js`, `.pdf`)
- **Stage 2: Text Chunking** - Smart chunking (500 chars) with sentence boundary detection and overlap
- **Stage 3: Embedding Generation** - Gemini `text-embedding-004` model (768 dimensions)
- **Stage 4: ChromaDB Indexing** - Persistent vector storage with metadata

### ğŸ’¬ Interactive Chat Interface
- **Multi-collection support** - Organize documents into separate collections
- **Semantic search** - Find relevant chunks by meaning, not just keywords
- **AI integration** - Ask questions about search results using Gemini
- **Conversation history** - Persistent SQLite storage

### ğŸ“š Collection Management
- Create named collections for different document sets
- Search within specific collections or across all
- List collections with statistics
- Delete individual collections or clear all data

## Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Files/Directories       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Document Loader       â”‚
â”‚  â€¢ Load .md, .rst, .txt, .py    â”‚
â”‚  â€¢ Extract metadata             â”‚
â”‚  â€¢ Handle encoding              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Text Chunker          â”‚
â”‚  â€¢ Split into 500-char chunks   â”‚
â”‚  â€¢ 50-char overlap              â”‚
â”‚  â€¢ Sentence boundary detection  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Embedding Generator   â”‚
â”‚  â€¢ Gemini text-embedding-004    â”‚
â”‚  â€¢ Batch processing (100/batch) â”‚
â”‚  â€¢ 768-dimensional vectors      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4: ChromaDB Index        â”‚
â”‚  â€¢ Store vectors + metadata     â”‚
â”‚  â€¢ Persistent storage           â”‚
â”‚  â€¢ Fast similarity search       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Searchable Index       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
day19/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ document_loader.py        # Stage 1: File loading (.md, .rst, .txt, .py, .pdf)
â”‚   â”œâ”€â”€ text_chunker.py           # Stage 2: Text chunking
â”‚   â”œâ”€â”€ embedding_generator.py    # Stage 3: Gemini embeddings
â”‚   â”œâ”€â”€ index_manager.py          # Stage 4: ChromaDB operations
â”‚   â”œâ”€â”€ pipeline_executor.py      # Orchestrator
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ gemini_client.py          # Gemini API client
â”‚   â”œâ”€â”€ conversation.py           # Chat history manager
â”‚   â”œâ”€â”€ storage.py                # SQLite persistence
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chroma_db/                # ChromaDB storage (generated)
â”‚   â””â”€â”€ conversations.db          # Chat history (generated)
â”œâ”€â”€ requests-docs/                # Git submodule: Requests library docs
â”‚   â””â”€â”€ docs/                     # 15 .rst files with documentation
â”œâ”€â”€ chat.py                       # Main interactive CLI
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Installation

1. **Clone repository with submodules:**
```bash
cd day19
git submodule update --init --recursive
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Set Gemini API key:**
```bash
export GEMINI_API_KEY="your-api-key-here"
```

## Quick Start

```bash
# Start chat interface
python3 chat.py

# Index the Requests documentation (15 .rst files)
You: /index requests-docs/docs --collection requests_docs

# Search for topics
You: /search "how to make HTTP requests"
You: /search "authentication methods"
```

## Usage

### Commands Reference

#### Document Indexing
```bash
/index <path> [--collection <name>]
```

**Examples:**
```
# Index Requests documentation
/index requests-docs/docs --collection requests_docs

# Index with auto-generated collection name
/index requests-docs/docs

# Index single file
/index requests-docs/docs/user/quickstart.rst
```

#### Searching
```bash
/search <query> [--collection <name>]
```

**Examples:**
```
# Search all collections
/search "making HTTP requests"

# Search specific collection
/search "authentication" --collection requests_docs

# Natural language queries
/search "How to handle errors and exceptions?"
/search "What are session objects?"
/search "How to send POST requests with JSON?"
```

#### Collection Management
```bash
/list-collections              # Show all collections
/delete-collection <name>      # Delete specific collection
/clear-index                   # Clear ALL data (requires confirmation)
```

#### Chat Commands
```bash
/resume      # Load previous conversation
/clear       # Start new conversation
/model       # Change AI model
/help        # Show all commands
/quit        # Exit
```

## Example Workflow

```bash
$ python3 chat.py

================================================================
     DOCUMENT INDEXING CHAT - AI Assistant
================================================================

You: /index requests-docs/docs --collection requests_docs

ğŸ“š Indexing: requests-docs/docs
Collection: requests_docs
======================================================================
ğŸ“„ Stage 1: Loading documents...
  âœ“ Loaded 15 documents (52840 chars)
âœ‚ï¸  Stage 2: Chunking text...
  âœ“ Created 68 chunks (avg: 490 chars)
ğŸ§  Stage 3: Generating embeddings...
  Processing batch 1/1 (68 chunks)...
  âœ“ Generated 68 embeddings (100.0% success)
ğŸ’¾ Stage 4: Indexing in ChromaDB...
  âœ“ Indexed 68 chunks in collection 'requests_docs'

======================================================================
âœ“ Indexing complete!
  Documents loaded: 15
  Chunks created: 68
  Embeddings generated: 68
  Documents indexed: 68

ğŸ’¡ Try: /search "your query" --collection requests_docs

You: /search "How to make HTTP requests?"

ğŸ” Searching for: "How to make HTTP requests?"
Collection: All collections
======================================================================

ğŸ“‹ Found 5 results:

â•­â”€ Result 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Making Requests                               â”‚
â”‚ ===============                               â”‚
â”‚                                               â”‚
â”‚ Making a request with Requests is very        â”‚
â”‚ simple. Begin by importing the Requests       â”‚
â”‚ module::                                      â”‚
â”‚                                               â”‚
â”‚     >>> import requests                       â”‚
â”‚                                               â”‚
â”‚ Now, let's try to get a webpage...           â”‚
â”‚                                               â”‚
â”‚ Source: quickstart.rst                        â”‚
â”‚ Chunk: 3 | Distance: 0.2156                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

[More results...]

ğŸ’¡ You can now ask AI questions about these results!

You: Show me examples of POST requests

AI: Based on the documentation, here are examples
    of making POST requests with the Requests library:

    1. Simple POST request:
    >>> r = requests.post('https://httpbin.org/post',
                          data={'key':'value'})

    2. Sending JSON data:
    >>> r = requests.post('https://httpbin.org/post',
                          json={'key': 'value'})

    [AI continues with detailed explanation...]

You: /list-collections

ğŸ“š Collections
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name           â”‚ Documents â”‚ Metadata            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ requests_docs  â”‚ 68        â”‚ source_path: requ.. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 1 collections, 68 documents
```

## Technical Details

### Pipeline Configuration
- **Chunk Size**: 500 characters
- **Overlap**: 50 characters  
- **Embedding Model**: `text-embedding-004`
- **Embedding Dimensions**: 768
- **Batch Size**: 100 chunks per API call
- **Vector DB**: ChromaDB with persistence

### Supported File Types
- Markdown (`.md`)
- reStructuredText (`.rst`) - **Sphinx/Python docs**
- Text files (`.txt`)
- Python (`.py`)
- JavaScript (`.js`)
- Java (`.java`)
- Go (`.go`)
- Rust (`.rs`)
- C++ (`.cpp`)
- PDF (`.pdf`) - requires PyPDF2

### API Rate Limits
- Gemini Free Tier: 1,500 embedding requests/day
- 1 request = 1 batch (up to 100 texts)
- **Requests docs**: 15 files â†’ ~68 chunks (well within limits!)

### Performance
- **Loading**: ~50ms per file
- **Chunking**: ~100ms for 10k chars
- **Embeddings**: 1-2s per batch (100 chunks)
- **Indexing**: ~50ms per batch
- **Search**: <100ms for 1M vectors

## Advanced Usage

### Batch Indexing Multiple Directories

```bash
# Index Requests documentation
/index requests-docs/docs --collection requests_docs

# Index your own project documentation
/index my-project/docs --collection my_docs

# Search across all collections
/search "authentication examples"

# Search specific collection
/search "authentication" --collection requests_docs
```

### Custom Chunking Strategy

Modify `pipeline/text_chunker.py`:

```python
# Larger chunks for technical docs
chunker = TextChunker(chunk_size=1000, overlap=100)

# Smaller chunks for Q&A
chunker = TextChunker(chunk_size=300, overlap=30)
```

### Metadata Filtering

Search with metadata filters (modify `index_manager.py`):

```python
results = index_manager.search(
    query_embedding=embedding,
    where={"file_type": "py"},  # Only Python files
    top_k=5
)
```

## Troubleshooting

### "Module not found" errors
```bash
pip install -r requirements.txt
```

### "API key not set"
```bash
export GEMINI_API_KEY="your-key"
# Or set in ~/.bashrc or ~/.zshrc
```

### "Rate limit exceeded"
- Wait 24 hours for quota reset
- Reduce chunk count (index fewer documents)
- Use smaller chunk sizes

### "No search results"
- Check collection name spelling
- Try broader search queries
- Verify documents were indexed: `/list-collections`

### ChromaDB errors
```bash
# Clear and restart
rm -rf data/chroma_db
python3 chat.py
```

## Learning Outcomes

This project demonstrates:

1. **Document Processing Pipeline**: Multi-stage data transformation
2. **Text Chunking Strategies**: Overlap and boundary detection
3. **Embedding Generation**: Converting text to vectors
4. **Vector Databases**: ChromaDB for similarity search
5. **Semantic Search**: Meaning-based retrieval vs keywords
6. **Collection Management**: Organizing vector spaces
7. **RAG Foundation**: Building blocks for retrieval systems

## What is RAG?

**RAG (Retrieval-Augmented Generation)** combines:
- **Retrieval**: Find relevant documents (what we built!)
- **Augmentation**: Add documents to LLM context
- **Generation**: LLM answers using retrieved info

This project completes the **Retrieval** component. Add LLM context injection for full RAG!

## Next Steps

To build full RAG:
1. âœ… **Indexing** (this project)
2. Search for relevant chunks
3. Inject chunks into LLM prompt
4. Generate answer with context

Example RAG flow:
```python
# 1. Search (already implemented)
results = pipeline.search("How to use async in Python?")

# 2. Build context
context = "\n\n".join([r.text for r in results])

# 3. Create prompt with context
prompt = f"""Based on these documents:
{context}

Answer: How to use async in Python?"""

# 4. Generate answer
response = gemini.generate_content(prompt)
```

## References

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Google Gemini API](https://ai.google.dev/)
- [Sentence Transformers](https://www.sbert.net/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

---

## License

Educational project for AI Advent Challenge
