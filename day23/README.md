# Day 23: Mini-Chat with RAG Memory and Automatic Source Citations

## Overview

A mini-chat with **automatic RAG (Retrieval-Augmented Generation)** that searches the document database for every user message and provides mandatory source citations. 

**New in this version**: Modular architecture with specialized managers and mandatory reranking for improved relevance.

## Key Features

### ğŸ¯ Automatic RAG Integration
- **No command needed** - RAG automatically activates for every message
- **Automatic context search** - Searches knowledge base before each response
- **Mandatory reranking** - Uses Gemini to score and filter search results
- **Mandatory source citations** - Every response shows which documents were used
- **Conversation history** - Full dialog persistence with SQLite
- **Seamless experience** - Just type your questions naturally

### ğŸ—ï¸ Modular Architecture
The code has been refactored into specialized managers:
- **IndexManager** - Document indexing and search operations
- **RagManager** - RAG search, context formatting, and Koog loading
- **DialogManager** - Conversation history and dialog management
- **SettingsManager** - System instructions and generation settings
- **UIManager** - Display, welcome screens, and model selection

**Result**: `chat.py` reduced from 1062 to 297 lines (72% reduction)

### ğŸ¯ Mandatory Reranking Pipeline
Every search now goes through a 2-stage retrieval process:

1. **Initial Retrieval**: ChromaDB returns 20 candidates using L2 distance
2. **Reranking**: Gemini scores each candidate (0-10) for relevance
3. **Filtering**: Only top-K results with score â‰¥ 5.0 are used

**Benefits:**
- âœ… Higher precision - irrelevant results filtered out
- âœ… Better ranking - semantic relevance vs. just embedding distance
- âœ… Quality control - low-quality matches automatically removed

### ğŸ“š Citation Format
All responses include inline citations:
```
"The A2A protocol uses JSON-RPC for communication [Source: a2a-client/Module.md, Chunk 2].
It enables standardized communication between AI agents [Source: a2a-core/Module.md, Chunk 3]."
```

After each response, sources are displayed with rerank scores:
```
ğŸ“š Sources (3 documents):
  1. a2a-client/Module.md (Chunk 2, relevance: 0.85, rerank: 8.5/10)
  2. a2a-core/Module.md (Chunk 3, relevance: 0.82, rerank: 7.8/10)
  3. README.md (Chunk 1, relevance: 0.79, rerank: 7.2/10)
```

### âœ… Benefits
1. **Transparency** - Users can verify every claim
2. **Accuracy** - Reranking + citations eliminate hallucinations
3. **Trust** - Clear evidence with quality scores for all statements
4. **Traceability** - Direct links to source documents
5. **Quality Control** - Easy to identify missing or weak sources
6. **Maintainability** - Modular code is easier to extend and test

## How It Works

### Automatic RAG Pipeline with Reranking

```
User types message
    â†“
1. Save message to conversation history
    â†“
2. AUTOMATIC: Search knowledge base
   - Generate embedding (Ollama mxbai-embed-large, 1024d)
   - Search ChromaDB for top-20 candidates (L2 distance)
    â†“
3. AUTOMATIC: Rerank with Gemini
   - Score each candidate 0-10 for relevance
   - Filter by distance threshold (< 195.0)
   - Keep only scores â‰¥ 5.0
   - Return top-5 best matches
    â†“
4. AUTOMATIC: Build enhanced prompt
   [RELEVANT DOCUMENTATION CONTEXT]
   [Source 1: file1.md, Chunk 2]
   Content from file1...

   [Source 2: file2.md, Chunk 5]
   Content from file2...

   Please cite sources in your response using the format [Source: filename, Chunk X].

   Question: User's original question...
    â†“
5. Send to Gemini with conversation history
    â†“
6. Display response
    â†“
7. AUTOMATIC: Display sources with rerank scores
   ğŸ“š Sources (5 documents):
     1. file1.md (Chunk 2, relevance: 0.85, rerank: 8.5/10)
     2. file2.md (Chunk 5, relevance: 0.82, rerank: 7.8/10)
     ...
```

### Citation Guidelines
The system uses soft citation guidance:

```
Use the following documentation to answer the question.
Please cite sources in your response using the format [Source: filename, Chunk X].

Documentation:
---
[CONTEXT CHUNKS]
---

Question: [USER QUESTION]
```

## Usage

### Prerequisites
```bash
# Install Ollama
brew install ollama  # macOS
# or download from https://ollama.com

# Start Ollama server
ollama serve

# Pull embedding model
ollama pull mxbai-embed-large
```

### Start Chat
```bash
cd day23
python chat.py
```

### First Time: Load Documentation
```bash
/load-koog
```
Loads Koog documentation using Ollama embeddings. This is cached - only needs to run once.

### Just Type Your Questions!
No special command needed - RAG is automatic:
```bash
You: What is the A2A protocol and what is it used for?
```

**Output:**
```
AI: The A2A (Agent-to-Agent) protocol is a standardized
communication protocol for AI agents [Source: AGENT.md, Chunk 1].
It enables seamless communication between different AI agents in
distributed systems using a JSON-RPC based transport layer
[Source: a2a-core/Module.md, Chunk 2].


ğŸ“š Sources (3 documents):
  1. AGENT.md (Chunk 1, relevance: 0.85, rerank: 8.5/10)
  2. a2a-core/Module.md (Chunk 2, relevance: 0.82, rerank: 7.8/10)
  3. a2a-client/Module.md (Chunk 3, relevance: 0.76, rerank: 7.1/10)
```

**Key difference**: Sources with rerank scores are automatically displayed after EVERY response!

### View Documentation Info
```bash
/koog-info
```
Shows loaded documents and suggests sample questions.

### Available Commands
```
Document Management:
  /load-koog [--force]  - Load Koog docs (cached, use --force to reindex)
  /koog-info            - Show loaded docs & suggest questions
  /index <path>         - Index custom documents
  /search <query>       - Search index manually
  /list-collections     - Show all collections

Chat Commands:
  /resume    - Load previous dialog
  /clear     - Delete current dialog & create new
  /model     - Change Gemini model
  /system    - View/change system instruction
  /settings  - View/change generation settings
  /compress  - Compress conversation history
  /tokens    - Show token statistics
  /help      - Show all commands
  /quit      - Exit

Note: No /ask command needed - RAG is automatic for all messages!
```

## Technical Details

### Architecture
```
day23/
â”œâ”€â”€ chat.py                      # Main orchestrator (297 lines, was 1062!)
â”‚   â”œâ”€â”€ handle_command()         # Command routing
â”‚   â”œâ”€â”€ chat_loop()              # Main loop with automatic RAG
â”‚   â””â”€â”€ _init_rag_manager()      # Lazy initialization
â”œâ”€â”€ managers/                    # NEW: Modular architecture
â”‚   â”œâ”€â”€ __init__.py              # Manager exports
â”‚   â”œâ”€â”€ index_manager.py         # Document indexing (258 lines)
â”‚   â”œâ”€â”€ rag_manager.py           # RAG operations (319 lines)
â”‚   â”œâ”€â”€ dialog_manager.py        # Conversation mgmt (208 lines)
â”‚   â”œâ”€â”€ settings_manager.py      # Settings (107 lines)
â”‚   â””â”€â”€ ui_manager.py            # UI/Display (144 lines)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ conversation.py          # Dialog history with SQLite
â”‚   â”œâ”€â”€ gemini_client.py         # Gemini API client
â”‚   â”œâ”€â”€ storage.py               # SQLite operations
â”‚   â””â”€â”€ text_manager.py          # Token counting
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ rag_client.py            # NEW: Simple RAG client with reranking
â”‚   â”œâ”€â”€ reranker.py              # Gemini-based reranker
â”‚   â””â”€â”€ koog_loader.py           # Koog documentation loader
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ ollama_embedding_generator.py  # Ollama embeddings (1024d)
â”‚   â”œâ”€â”€ index_manager.py         # ChromaDB operations
â”‚   â”œâ”€â”€ document_loader.py       # Document loading
â”‚   â”œâ”€â”€ text_chunker.py          # Text chunking
â”‚   â””â”€â”€ pipeline_executor.py     # Indexing pipeline
â”œâ”€â”€ koog/                        # Git submodule with docs
â””â”€â”€ data/
    â”œâ”€â”€ conversations.db         # SQLite dialog storage
    â””â”€â”€ chroma_db/               # Persisted vector database
```

### Key Components

#### RagClient (rag/rag_client.py)
Simple client that coordinates search and reranking:
```python
class RagClient:
    def search(self, question: str, top_k: int = 5, initial_k: int = 20):
        # 1. Generate embedding
        question_embedding = self.embedding_generator.generate_embedding(question)

        # 2. Get initial candidates from ChromaDB
        initial_chunks = self.index_manager.search(
            query_embedding=question_embedding,
            top_k=initial_k  # Get 20 candidates
        )

        # 3. Rerank with Gemini
        reranked_chunks, stats = self.reranker.rerank(
            query=question,
            chunks=initial_chunks,
            top_k=top_k  # Return top 5
        )

        return reranked_chunks
```

#### Reranker (rag/reranker.py)
Three-stage filtering pipeline:
```python
class Reranker:
    def rerank(self, query: str, chunks: list, top_k: int = 3):
        # Stage 1: Filter by distance threshold
        filtered = [c for c in chunks if c.distance < 195.0]

        # Stage 2: Score with Gemini (0-10)
        for chunk in filtered:
            chunk.rerank_score = self._score_relevance(query, chunk.text)

        # Stage 3: Filter by min score and sort
        final = [c for c in filtered if c.rerank_score >= 5.0]
        final.sort(key=lambda x: x.rerank_score, reverse=True)

        return final[:top_k]
```

#### RagManager (managers/rag_manager.py)
Handles all RAG operations:
- `load_koog()` - Load and index Koog documentation
- `show_koog_info()` - Display documentation statistics
- `init_rag_client()` - Initialize RAG client with reranker
- `is_rag_available()` - Check if RAG collection exists
- `search_context()` - Search with automatic reranking
- `format_context_for_prompt()` - Format chunks for LLM

### Technologies
- **LLM**: Google Gemini 2.5 Flash
- **Reranking**: Google Gemini 2.5 Flash Lite (for speed)
- **Embeddings**: Ollama mxbai-embed-large (1024d, local)
- **Vector DB**: ChromaDB (L2 distance)
- **Dataset**: Koog documentation (165 markdown files)
- **UI**: Rich (terminal with markdown rendering)
- **Storage**: SQLite (conversation persistence)

### Reranking Configuration
```python
Reranker(
    gemini_client=gemini_client,
    distance_threshold=195.0,    # L2 distance for 1024d vectors
    min_rerank_score=5.0,        # Minimum relevance score (0-10)
    max_score=10.0               # Maximum score
)
```

## Success Metrics

### Code Quality
- âœ… **Reduction**: 72% fewer lines in main file (1062 â†’ 297)
- âœ… **Modularity**: 5 specialized managers with clear responsibilities
- âœ… **Maintainability**: Easier to test and extend
- âœ… **Separation**: UI, RAG, Dialog, Settings, and Index concerns separated

### RAG Quality
- âœ… **Precision**: Reranking improves top-K accuracy
- âœ… **Relevance**: Gemini scoring catches semantic mismatches
- âœ… **Coverage**: 100% of answers contain citations
- âœ… **Format**: All citations match `[Source: filename, Chunk X]`

### Hallucination Reduction
**Without Reranking:**
- May include marginally relevant chunks
- Distance-based ranking sometimes misleading
- Lower quality results reduce answer accuracy

**With Mandatory Reranking:**
- âœ… Semantic relevance verified by LLM
- âœ… Low-quality matches filtered out
- âœ… Better context quality â†’ fewer hallucinations
- âœ… Transparent quality scores shown to user

## Example Interaction

```
You: /load-koog
âœ“ Koog collection already loaded from cache
ğŸ’¡ Just type your questions - RAG is automatic!

You: What is the A2A protocol?

AI: The A2A (Agent-to-Agent) protocol is a standardized protocol for
communication between AI agents [Source: AGENT.md, Chunk 1]. It defines
how agents discover each other, exchange messages, and coordinate tasks
[Source: a2a-core/Module.md, Chunk 2]. The protocol uses JSON-RPC as
its transport mechanism [Source: a2a-transport/README.md, Chunk 3].


ğŸ“š Sources (3 documents):
  1. AGENT.md (Chunk 1, relevance: 0.85, rerank: 8.5/10)
  2. a2a-core/Module.md (Chunk 2, relevance: 0.82, rerank: 7.8/10)
  3. a2a-transport/README.md (Chunk 3, relevance: 0.79, rerank: 7.2/10)

You: /quit
Goodbye!
```

## Troubleshooting

### Ollama Connection Error
```
Connection error: Is Ollama running at http://localhost:11434?
```
**Solution**: Start Ollama server
```bash
ollama serve
ollama pull mxbai-embed-large
```

### Gemini API Overloaded (503)
```
API error (status 503): The model is overloaded. Please try again later.
```
**Solution**: Wait a few moments or try a different model
```bash
/model
# Select option 2 (Flash Lite - usually less loaded)
```

### No Documents Found
```
â„¹ï¸  RAG not loaded. Answer based on general knowledge.
ğŸ’¡ Use /load-koog to enable document search
```
**Solution**: Load documentation
```bash
/load-koog
```

## References

- [Ollama](https://ollama.com/) - Local LLM runtime
- [mxbai-embed-large](https://ollama.com/library/mxbai-embed-large) - Embedding model on Ollama
- [Koog](https://github.com/JetBrains/koog) - JetBrains AI framework
- [RAG Paper](https://arxiv.org/abs/2005.11401) - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
- [ChromaDB Documentation](https://docs.trychroma.com/) - Vector database for embeddings
- [Google Gemini API](https://ai.google.dev/) - Gemini models and embeddings
- [Reranking in RAG](https://arxiv.org/abs/2407.21439) - Survey on reranking techniques for RAG
- [RAG Survey](https://arxiv.org/abs/2312.10997) - Recent survey on RAG techniques

---

## License

Educational project for AI Advent Challenge
