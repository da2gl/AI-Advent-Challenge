"""Demo mode with predefined prompts for testing token compression."""

from typing import Dict, List
from rich.spinner import Spinner
from rich.live import Live

from conversation import ConversationHistory
from text_manager import TextManager


class DemoPrompts:
    """Predefined prompts for demo mode."""

    SHORT = "Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ? ÐžÐ±ÑŠÑÑÐ½Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾."

    LONG = """ÐžÐ±ÑŠÑÑÐ½Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¾ÑÐ½Ð¾Ð²Ñ‹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ:

1. Ð¢Ñ€Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ñ‚Ð¸Ð¿Ð° ML (supervised, unsupervised, reinforcement)
2. ÐšÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ðµ ÑÐµÑ‚Ð¸ (Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹)
3. ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ
4. ÐžÑ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
5. ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ (TensorFlow, PyTorch, scikit-learn)
6. ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸
7. ÐŸÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ vs Ð½ÐµÐ´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
8. ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°

ÐžÑ‚Ð²ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼, Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸ÑÐ¼Ð¸."""

    VERY_LONG = (
        "ÐœÐ½Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ð¸ ÐºÐ¾Ð´Ð¾Ð¼. "
        "Ð’Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð² Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ: ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Machine Learning, ML) - ÑÑ‚Ð¾ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ "
        "Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð´Ð°ÐµÑ‚ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð°Ð¼ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒÑÑ Ð±ÐµÐ· ÑÐ²Ð½Ð¾Ð³Ð¾ "
        "Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ. Ð’Ð¼ÐµÑÑ‚Ð¾ Ñ‚Ð¾Ð³Ð¾ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ, Ð¼Ñ‹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÐ¼ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð°Ð¼ "
        "Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾. "
        "Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¸ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ðµ: ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ ÑƒÐ¶Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð´ÐµÑÑÑ‚Ð¸Ð»ÐµÑ‚Ð¸Ð¹. ÐŸÐµÑ€Ð²Ñ‹Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ "
        "Ð½Ð°Ñ‡Ð°Ð»Ð¸ÑÑŒ Ð² 1950-Ñ… Ð³Ð¾Ð´Ð°Ñ…, ÐºÐ¾Ð³Ð´Ð° ÐÑ€Ñ‚ÑƒÑ€ Ð¡ÑÐ¼ÑŽÑÐ» ÑÐ¾Ð·Ð´Ð°Ð» Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ð´Ð»Ñ Ð¸Ð³Ñ€Ñ‹ Ð² ÑˆÐ°ÑˆÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ "
        "ÑƒÑ‡Ð¸Ð»Ð°ÑÑŒ Ð½Ð° ÑÐ²Ð¾ÐµÐ¼ Ð¾Ð¿Ñ‹Ñ‚Ðµ. Ð¡ Ñ‚ÐµÑ… Ð¿Ð¾Ñ€ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ€Ð°Ð·Ð²Ð¸Ð»Ð°ÑÑŒ, Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ñ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸ÐµÐ¼ "
        "Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¼Ð¾Ñ‰Ð½Ñ‹Ñ… Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð². "
        "Ð¢Ð¸Ð¿Ñ‹ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ: "
        "1. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ (Supervised Learning) - ÑÑ‚Ð¾ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚Ð¸Ð¿ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, "
        "Ð³Ð´Ðµ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ñ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ñ… Ñ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¼Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ð¼Ð¸. "
        "ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð·Ð°Ð´Ð°Ñ‡: ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ (ÐºÐ¾ÑˆÐºÐ¸ vs ÑÐ¾Ð±Ð°ÐºÐ¸), ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ†ÐµÐ½ Ð½Ð° Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚ÑŒ, "
        "Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÑƒÐºÐ¾Ð¿Ð¸ÑÐ½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°, Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÐ¿Ð°Ð¼Ð° Ð² ÑÐ»ÐµÐºÑ‚Ñ€Ð¾Ð½Ð½Ð¾Ð¹ Ð¿Ð¾Ñ‡Ñ‚Ðµ, ÐœÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ°Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°. "
        "ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹: Ð›Ð¸Ð½ÐµÐ¹Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ, Ð›Ð¾Ð³Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ, Ð”ÐµÑ€ÐµÐ²ÑŒÑ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹, "
        "Ð¡Ð»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¹ Ð»ÐµÑ (Random Forest), ÐœÐµÑ‚Ð¾Ð´ Ð¾Ð¿Ð¾Ñ€Ð½Ñ‹Ñ… Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² (SVM), ÐÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ðµ ÑÐµÑ‚Ð¸. "
        "2. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð±ÐµÐ· ÑƒÑ‡Ð¸Ñ‚ÐµÐ»Ñ (Unsupervised Learning) - Ð·Ð´ÐµÑÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ñ‹, Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¾Ð»Ð¶Ð½Ð° "
        "ÑÐ°Ð¼Ð° Ð½Ð°Ð¹Ñ‚Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð·Ð°Ð´Ð°Ñ‡: ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð², Ð£Ð¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…, "
        "ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹, Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹. ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ñ‹: K-means ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ, "
        "Ð˜ÐµÑ€Ð°Ñ€Ñ…Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ, PCA (ÐœÐµÑ‚Ð¾Ð´ Ð³Ð»Ð°Ð²Ð½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚), ÐÐ²Ñ‚Ð¾ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ñ‹. "
        "3. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (Reinforcement Learning) - Ð°Ð³ÐµÐ½Ñ‚ ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ, Ð¿Ð¾Ð»ÑƒÑ‡Ð°Ñ "
        "Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð¸Ð»Ð¸ ÑˆÑ‚Ñ€Ð°Ñ„Ñ‹ Ð·Ð° ÑÐ²Ð¾Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ. ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ: Ð˜Ð³Ñ€Ð¾Ð²Ñ‹Ðµ AI (AlphaGo, ÑˆÐ°Ñ…Ð¼Ð°Ñ‚Ñ‹), "
        "Ð Ð¾Ð±Ð¾Ñ‚Ð¾Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ°, ÐÐ²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾Ð¼Ð¾Ð±Ð¸Ð»Ð¸, Ð¢Ð¾Ñ€Ð³Ð¾Ð²Ñ‹Ðµ Ð±Ð¾Ñ‚Ñ‹. "
        "ÐÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ðµ ÑÐµÑ‚Ð¸ Ð¸ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ: ÐÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ðµ ÑÐµÑ‚Ð¸ Ð²Ð´Ð¾Ñ…Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¾Ð¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¼Ð¾Ð·Ð³Ð°. "
        "ÐžÐ½Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÑ‚ Ð¸Ð· ÑÐ»Ð¾ÐµÐ² Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾. "
        "ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹: Ð’Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹ (Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ), Ð¡ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ ÑÐ»Ð¾Ð¸ (Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ), "
        "Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ ÑÐ»Ð¾Ð¹ (Ð´Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚), Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ (Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑŽÑ‚ Ð½ÐµÐ»Ð¸Ð½ÐµÐ¹Ð½Ð¾ÑÑ‚ÑŒ), "
        "Ð’ÐµÑÐ° Ð¸ ÑÐ¼ÐµÑ‰ÐµÐ½Ð¸Ñ (Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹). "
        "Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Deep Learning) - ÑÑ‚Ð¾ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ðµ ÑÐµÑ‚Ð¸ Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾Ð¼ ÑÐ»Ð¾ÐµÐ², ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ðµ Ñ€ÐµÑˆÐ°Ñ‚ÑŒ "
        "Ð¾Ñ‡ÐµÐ½ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸. "
        "ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸: "
        "1. Ð¡Ð±Ð¾Ñ€ Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… "
        "2. Ð Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÑƒÑŽ Ð¸ Ñ‚ÐµÑÑ‚Ð¾Ð²ÑƒÑŽ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ "
        "3. Ð’Ñ‹Ð±Ð¾Ñ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð³Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² "
        "4. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… "
        "5. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² "
        "6. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ð¾Ñ‚Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐµ "
        "7. Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ Ð² Ð¿Ñ€Ð¾Ð´Ð°ÐºÑˆÐ½. "
        "ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ: ÐŸÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Overfitting) - Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð·Ð°Ð¿Ð¾Ð¼Ð¸Ð½Ð°ÐµÑ‚ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, "
        "Ð¿Ð»Ð¾Ñ…Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ: Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ, dropout, Ð±Ð¾Ð»ÑŒÑˆÐµ Ð´Ð°Ð½Ð½Ñ‹Ñ…. "
        "ÐÐµÐ´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ (Underfitting) - Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ð°Ñ, Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ ÑƒÐ»Ð¾Ð²Ð¸Ñ‚ÑŒ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸, "
        "Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ: Ð±Ð¾Ð»ÐµÐµ ÑÐ»Ð¾Ð¶Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð². "
        "Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð¸ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸: Python - Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÑÐ·Ñ‹Ðº Ð´Ð»Ñ ML: TensorFlow - Ð¾Ñ‚ Google, PyTorch - Ð¾Ñ‚ Facebook, "
        "scikit-learn - Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼Ð¾Ð², Keras - Ð²Ñ‹ÑÐ¾ÐºÐ¾ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ñ‹Ð¹ API, pandas - Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸, "
        "NumPy - Ð´Ð»Ñ Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ñ… Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹. "
        "ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°: Ð”Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸: Accuracy (Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ), Precision (Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹), "
        "Recall (Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ð°), F1-score. Ð”Ð»Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¸: MSE (ÑÑ€ÐµÐ´Ð½ÐµÐºÐ²Ð°Ð´Ñ€Ð°Ñ‚Ð¸Ñ‡Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°), "
        "MAE (ÑÑ€ÐµÐ´Ð½ÑÑ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°), RÂ² (ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ñ†Ð¸Ð¸). "
        "ÐŸÑ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÑÐ¾Ð²ÐµÑ‚Ñ‹: "
        "1. ÐÐ°Ñ‡Ð½Ð¸Ñ‚Ðµ Ñ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ "
        "2. Ð’ÑÐµÐ³Ð´Ð° Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ "
        "3. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÐºÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ "
        "4. Ð¡Ð»ÐµÐ´Ð¸Ñ‚Ðµ Ð·Ð° Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ "
        "5. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ñ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸ "
        "6. Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹. "
        "Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð²ÑÐµ ÑÑ‚Ð¾ ÐµÑ‰Ðµ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½ÐµÐµ, Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ ÐºÐ¾Ð´Ð° Ð½Ð° Python, Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸ Ð¿Ñ€Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ "
        "Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñ‹ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹ (CNN, RNN, Transformer), Ð¾Ð±ÑŠÑÑÐ½Ð¸ ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ backpropagation, "
        "Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº, Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ñ‹ Ñ‚Ð¸Ð¿Ð° Adam Ð¸ SGD. Ð¢Ð°ÐºÐ¶Ðµ Ñ…Ð¾Ñ‡Ñƒ ÑƒÐ·Ð½Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ transfer learning, "
        "data augmentation, batch normalization, Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸."
    )

    @classmethod
    def get_prompts(cls) -> List[Dict[str, str]]:
        """Get all demo prompts with metadata.

        Returns:
            List of dictionaries with prompt info
        """
        return [
            {"name": "short", "title": "Short prompt (~50 tokens) â†’ 2048 tokens response", "text": cls.SHORT},
            {"name": "long", "title": "Long prompt (~246 tokens) â†’ 2048 tokens response", "text": cls.LONG},
            {"name": "very_long", "title": "Very long prompt (~1600 tokens) â†’ 2048 tokens response", "text": cls.VERY_LONG},
        ]


class DemoMode:
    """Demo mode for testing token compression with predefined prompts."""

    # Special system instruction for demo mode - ensures short responses
    DEMO_SYSTEM_INSTRUCTION = (
        "Ð¢Ñ‹ â€” AI Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼Ð¸. "
        "Ð’ÐÐ–ÐÐž: Ð”Ð°Ð²Ð°Ð¹ ÐšÐ ÐÐ¢ÐšÐ˜Ð• Ð¸ Ð¡Ð–ÐÐ¢Ð«Ð• Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ (Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 500-700 Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²). "
        "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¿Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ñƒ, Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹. "
        "Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€ÑƒÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚ ÐºÑ€Ð°Ñ‚ÐºÐ¾: Ð¾ÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð¼Ñ‹ÑÐ»ÑŒ â†’ 2-3 ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ð¿ÑƒÐ½ÐºÑ‚Ð° â†’ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´."
    )

    def __init__(self, client, conversation, console):
        """Initialize demo mode.

        Args:
            client: GeminiApiClient instance
            conversation: ConversationHistory instance
            console: Rich Console instance
        """
        self.client = client
        self.conversation = conversation
        self.console = console
        self.original_limits = {}

    def run(self, current_model, system_instruction, temperature, top_k, top_p, max_output_tokens):
        """Run demo mode with predefined prompts.

        Args:
            current_model: Current Gemini model
            system_instruction: System instruction for AI
            temperature: Generation temperature
            top_k: Top K parameter
            top_p: Top P parameter
            max_output_tokens: Max output tokens
        """
        self._display_welcome()
        self._setup_demo_limits()

        try:
            self._run_demo_loop(
                current_model,
                system_instruction,
                temperature,
                top_k,
                top_p,
                max_output_tokens
            )
        finally:
            self._restore_limits()

    def _display_welcome(self):
        """Display demo mode welcome message."""
        self.console.print("\n" + "=" * 60, style="bright_magenta")
        self.console.print("ðŸŽ¬ DEMO MODE - Token Compression Demonstration", style="bold bright_magenta")
        self.console.print("=" * 60, style="bright_magenta")
        self.console.print("\nThis demo shows how compression works with reduced limits:", style="yellow")
        self.console.print("  â€¢ Context limit: 2,000 tokens (instead of 30,000)", style="dim")
        self.console.print("  â€¢ Warning threshold: 1,500 tokens (75%)", style="dim")
        self.console.print("  â€¢ Keep recent: 3 messages (instead of 5)", style="dim")
        self.console.print("  â€¢ Responses: Forced to be short (500-700 tokens)", style="dim")
        self.console.print("\nThe chat will quickly fill up and trigger compression warnings.", style="yellow")
        self.console.print("\n" + "=" * 60, style="bright_magenta")
        self.console.print("Available demo prompts:", style="yellow")

        for prompt in DemoPrompts.get_prompts():
            tokens = TextManager.estimate_tokens(prompt["text"])
            self.console.print(f"  â€¢ {prompt['name']}: {prompt['title']} (~{tokens} tokens)", style="dim")

        self.console.print("\n" + "=" * 60, style="bright_magenta")
        self.console.print("Commands:", style="yellow")
        self.console.print("  /short      - Send short demo prompt", style="dim")
        self.console.print("  /long       - Send long demo prompt", style="dim")
        self.console.print("  /very_long  - Send very long demo prompt", style="dim")
        self.console.print("  /exit_demo  - Exit demo mode", style="dim")
        self.console.print("=" * 60, style="bright_magenta")
        self.console.print()

    def _setup_demo_limits(self):
        """Store original limits and set demo limits."""
        self.original_limits = {
            "max": ConversationHistory.MAX_CONTEXT_TOKENS,
            "threshold": ConversationHistory.SAFE_THRESHOLD,
            "keep": ConversationHistory.KEEP_RECENT_MESSAGES,
        }

        ConversationHistory.MAX_CONTEXT_TOKENS = 2000
        ConversationHistory.SAFE_THRESHOLD = 1500
        ConversationHistory.KEEP_RECENT_MESSAGES = 3

        self.conversation.clear()

    def _restore_limits(self):
        """Restore original limits."""
        ConversationHistory.MAX_CONTEXT_TOKENS = self.original_limits["max"]
        ConversationHistory.SAFE_THRESHOLD = self.original_limits["threshold"]
        ConversationHistory.KEEP_RECENT_MESSAGES = self.original_limits["keep"]

        self.conversation.clear()

        self.console.print("\n" + "=" * 60, style="bright_magenta")
        self.console.print("âœ“ Exited demo mode - Normal limits restored", style="green")
        self.console.print("=" * 60, style="bright_magenta")

    def _run_demo_loop(self, current_model, system_instruction, temperature, top_k, top_p, max_output_tokens):
        """Run demo mode main loop.

        Args:
            current_model: Current Gemini model
            system_instruction: System instruction
            temperature: Generation temperature
            top_k: Top K parameter
            top_p: Top P parameter
            max_output_tokens: Max output tokens
        """
        while True:
            try:
                self.console.print("\n", end="")
                self.console.print("You: ", style="bold bright_blue", end="")
                user_input = input().strip()

                if not user_input:
                    continue

                # Check for exit
                if user_input.lower() == "/exit_demo":
                    break

                # Handle demo prompt commands
                prompt_text, adjusted_max_tokens = self._handle_demo_command(user_input)
                if prompt_text is None:
                    if user_input.startswith("/"):
                        self.console.print("[red]Unknown command. Use /short, /long, /very_long, or /exit_demo[/red]")
                    else:
                        # Regular user input
                        prompt_text = user_input
                        adjusted_max_tokens = max_output_tokens

                if prompt_text:
                    self._process_prompt(
                        prompt_text,
                        current_model,
                        system_instruction,
                        temperature,
                        top_k,
                        top_p,
                        adjusted_max_tokens or max_output_tokens
                    )

            except KeyboardInterrupt:
                print("\n\nDemo interrupted")
                break
            except Exception as e:
                print(f"\nâœ— Error: {str(e)}")
                print("Please try again or type /exit_demo to exit demo mode")

    def _handle_demo_command(self, command: str) -> tuple:
        """Handle demo prompt commands.

        Args:
            command: User command

        Returns:
            Tuple of (prompt_text, max_output_tokens) or (None, None) if command not recognized
        """
        command_lower = command.lower()

        # Map commands to prompts and recommended max_output_tokens
        # Long prompts get shorter responses to demonstrate compression faster
        prompts_map = {
            "/short": (DemoPrompts.SHORT, 2048),      # Short prompt â†’ normal response
            "/long": (DemoPrompts.LONG, 2048),        # Long prompt â†’ short response
            "/very_long": (DemoPrompts.VERY_LONG, 2048),  # Very long prompt â†’ very short response
        }

        if command_lower in prompts_map:
            prompt_text, max_tokens = prompts_map[command_lower]
            tokens = TextManager.estimate_tokens(prompt_text)
            self.console.print(
                f"[dim]â†’ Loading {command_lower[1:]} prompt "
                f"({tokens:,} tokens, max_output: {max_tokens:,})[/dim]"
            )
            return prompt_text, max_tokens

        return None, None

    def _process_prompt(
        self,
        prompt_text,
        current_model,
        system_instruction,
        temperature,
        top_k,
        top_p,
        max_output_tokens
    ):
        """Process a prompt and display response.

        Args:
            prompt_text: Prompt to send
            current_model: Gemini model
            system_instruction: System instruction (ignored - demo uses its own)
            temperature: Temperature
            top_k: Top K
            top_p: Top P
            max_output_tokens: Max output tokens

        Note:
            Demo mode always uses DEMO_SYSTEM_INSTRUCTION to ensure short responses.
        """
        from chat import ConsoleChat

        # Check if input text is too long and needs compression
        prompt_to_send = prompt_text
        if ConversationHistory.should_compress_input(prompt_text):
            input_tokens = TextManager.estimate_tokens(prompt_text)
            self.console.print(
                f"\n[yellow]âš ï¸  Input too long ({input_tokens:,} tokens)! Compressing...[/yellow]"
            )
            try:
                # Show spinner during input compression
                spinner = Spinner("dots", text="Compressing input...", style="yellow")
                with Live(spinner, console=self.console, transient=True):
                    summary_result = TextManager.summarize_text(
                        text=prompt_text,
                        client=self.client,
                        max_tokens=2000,  # Compress to max 2000 tokens
                        language="mixed",
                        timeout=15  # 15 second timeout (faster failure)
                    )
                prompt_to_send = summary_result['summary']
                self.console.print(
                    f"[green]âœ“ Compressed input: {input_tokens:,} â†’ "
                    f"{summary_result['summary_tokens']:,} tokens[/green]"
                )
            except Exception as e:
                self.console.print(f"[red]âœ— Compression failed: {str(e)}[/red]")
                self.console.print("[yellow]Sending original text...[/yellow]")

        # Add user message to history (use compressed version if available)
        self.conversation.add_user_message(prompt_to_send)

        # Send request to Gemini with demo-specific system instruction
        spinner = Spinner("dots", text="Thinking...", style="bright_magenta")
        with Live(spinner, console=self.console, transient=True):
            response = self.client.generate_content(
                prompt=prompt_to_send,
                model=current_model,
                conversation_history=self.conversation.get_history(),
                system_instruction=self.DEMO_SYSTEM_INSTRUCTION,  # Always use demo instruction
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                max_output_tokens=max_output_tokens
            )

        # Show response
        self.console.print("Assistant: ", style="bold bright_magenta", end="")
        assistant_text = self.client.extract_text(response)

        # Use ConsoleChat's _print_response method
        chat = ConsoleChat()
        chat.console = self.console
        chat._print_response(assistant_text)

        # Display token usage
        usage = self.client.extract_usage_metadata(response)
        if usage:
            self.conversation.add_tokens(usage['total_tokens'])

            self.console.print(
                f"\n[dim]Tokens: {usage['prompt_tokens']} prompt + "
                f"{usage['response_tokens']} response = "
                f"{usage['total_tokens']} total[/dim]"
            )

            # Show context with DEMO limits
            progress = TextManager.format_token_usage(
                self.conversation.total_tokens,
                ConversationHistory.MAX_CONTEXT_TOKENS
            )
            self.console.print(f"[dim]Context [DEMO]: {progress}[/dim]")

            # Auto-compress if approaching limit
            if self.conversation.should_compress():
                self.console.print(
                    "\n[yellow]âš ï¸  Context limit reached! Auto-compressing...[/yellow]"
                )
                try:
                    # Show spinner during compression API call
                    spinner = Spinner("dots", text="Compressing conversation history...", style="yellow")
                    with Live(spinner, console=self.console, transient=True):
                        result = self.conversation.compress_history(self.client)

                    if result['messages_compressed'] > 0:
                        self.console.print(
                            f"[green]âœ“ Compressed {result['messages_compressed']} messages, "
                            f"saved {result['tokens_saved']:,} tokens[/green]"
                        )
                    else:
                        self.console.print(
                            f"[dim]â„¹ {result.get('message', 'No compression needed')}[/dim]"
                        )
                except Exception as e:
                    self.console.print(
                        f"[red]âœ— Auto-compression failed: {str(e)}[/red]\n"
                        f"[dim]Try reducing message length or clearing history with /clear[/dim]"
                    )

        # Add assistant message to history
        self.conversation.add_assistant_message(assistant_text)
